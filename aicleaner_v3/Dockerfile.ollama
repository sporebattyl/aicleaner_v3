# Ollama Container for AICleaner v3
# Pre-configured with recommended models for vision and text analysis

FROM ollama/ollama:latest

# Set environment variables
ENV OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_MODELS_PATH=/data/models \
    OLLAMA_AUTO_DOWNLOAD=true \
    OLLAMA_KEEP_ALIVE=24h \
    OLLAMA_MAX_LOADED_MODELS=3 \
    OLLAMA_NUM_PARALLEL=2

# Create necessary directories
RUN mkdir -p /data/models /scripts /config

# Copy model setup scripts
COPY scripts/setup-models.sh /scripts/
COPY scripts/health-check.sh /scripts/
RUN chmod +x /scripts/*.sh

# Create model configuration file
RUN cat > /config/models.json << 'EOF'
{
  "recommended_models": [
    {
      "name": "llava:13b",
      "type": "vision",
      "description": "Vision model for image analysis",
      "quantization": "q4_0",
      "priority": 1
    },
    {
      "name": "mistral:7b",
      "type": "text",
      "description": "Text model for task generation",
      "quantization": "q4_0",
      "priority": 2
    },
    {
      "name": "llama2:7b",
      "type": "text",
      "description": "Alternative text model",
      "quantization": "q4_0",
      "priority": 3
    }
  ],
  "performance_settings": {
    "max_memory_gb": 4,
    "max_cpu_percent": 80,
    "timeout_seconds": 120
  }
}
EOF

# Create startup script
RUN cat > /scripts/start-ollama.sh << 'EOF'
#!/bin/bash
set -e

echo "Starting Ollama server..."

# Start Ollama server in background
ollama serve &
OLLAMA_PID=$!

# Wait for server to be ready
echo "Waiting for Ollama server to be ready..."
for i in {1..30}; do
    if curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "Ollama server is ready!"
        break
    fi
    echo "Waiting... ($i/30)"
    sleep 2
done

# Check if server is running
if ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
    echo "ERROR: Ollama server failed to start"
    exit 1
fi

# Download recommended models if auto-download is enabled
if [ "$OLLAMA_AUTO_DOWNLOAD" = "true" ]; then
    echo "Auto-downloading recommended models..."
    /scripts/setup-models.sh
fi

# Keep the container running
echo "Ollama server started successfully. PID: $OLLAMA_PID"
wait $OLLAMA_PID
EOF

RUN chmod +x /scripts/start-ollama.sh

# Health check for Ollama service
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD /scripts/health-check.sh || exit 1

# Expose Ollama API port
EXPOSE 11434

# Set proper ownership
RUN chown -R ollama:ollama /data /scripts /config

# Switch to ollama user
USER ollama

# Start Ollama with model setup
CMD ["/scripts/start-ollama.sh"]
