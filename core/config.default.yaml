# ------------------------------------------------------------------------------
# Default Configuration for AICleaner v3
#
# This file provides default settings. DO NOT EDIT THIS FILE.
# Instead, create a 'config.user.yaml' file to override these settings.
#
# CONFIGURATION MERGE STRATEGY:
# - Dictionary values: Deep-merged (user settings override defaults)
# - List values: Completely replaced (user lists replace default lists)
# - Environment variables: Processed first, then user config applied
#
# PRECEDENCE ORDER (highest to lowest):
# 1. config.user.yaml values
# 2. Environment variable substitutions (${VAR_NAME})
# 3. config.default.yaml values
#
# Secrets (like API keys) should be managed via environment variables.
# The application will substitute ${VAR_NAME} with the corresponding env var.
# ------------------------------------------------------------------------------

# -- General Application Settings --
general:
  # The active AI provider to use for processing.
  # Must match one of the keys under the 'ai_providers' section.
  # 'ollama' is the default for easy local setup without requiring API keys.
  active_provider: ollama
  # Log level for the application. Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_level: INFO

# -- AI Provider Configurations --
# Define settings for each supported AI provider here.
ai_providers:
  # Provider priority order for failover (highest to lowest priority)
  provider_priorities: ["ollama", "gemini", "openai", "anthropic"]
  
  # Model compatibility mapping for cross-provider failover
  model_compatibility_map:
    "gpt-4o": ["claude-3-5-sonnet-20241022", "gemini-2.5-pro", "mistral"]
    "claude-3-5-sonnet-20241022": ["gpt-4o", "gemini-2.5-pro", "llama3.2"]
    "gemini-2.5-pro": ["gpt-4o", "claude-3-5-sonnet-20241022", "mistral"]
    "llama3.2": ["mistral", "gemini-2.5-flash-lite", "gpt-3.5-turbo"]
  # OpenAI Configuration
  openai:
    # API key for OpenAI services.
    # IMPORTANT: Set this via the OPENAI_API_KEY environment variable.
    api_key: ${OPENAI_API_KEY}
    # Default model to use for OpenAI requests.
    default_model: gpt-4o
    # Available models and their specific settings.
    models:
      gpt-4o:
        max_tokens: 4096
        temperature: 0.7
        # Example of a provider-specific parameter
        response_format: { "type": "json_object" }
      gpt-3.5-turbo:
        max_tokens: 4096
        temperature: 0.8

  # Google Gemini Configuration
  gemini:
    api_key: ${GEMINI_API_KEY}
    default_model: gemini-2.5-pro
    models:
      gemini-2.5-pro:
        # Highest capability model for complex tasks
        safety_settings:
          - category: HARM_CATEGORY_HARASSMENT
            threshold: BLOCK_MEDIUM_AND_ABOVE
          - category: HARM_CATEGORY_HATE_SPEECH
            threshold: BLOCK_MEDIUM_AND_ABOVE
        generation_config:
          temperature: 0.8
          top_p: 1.0
          max_output_tokens: 8192
      gemini-2.5-flash:
        # Fast model for real-time tasks, good balance of speed and capability
        safety_settings:
          - category: HARM_CATEGORY_HARASSMENT
            threshold: BLOCK_MEDIUM_AND_ABOVE
          - category: HARM_CATEGORY_HATE_SPEECH
            threshold: BLOCK_MEDIUM_AND_ABOVE
        generation_config:
          temperature: 0.7
          top_p: 0.95
          max_output_tokens: 8192
      gemini-2.5-flash-lite:
        # Lightweight model for simple tasks, fastest response times
        safety_settings:
          - category: HARM_CATEGORY_HARASSMENT
            threshold: BLOCK_MEDIUM_AND_ABOVE
          - category: HARM_CATEGORY_HATE_SPEECH
            threshold: BLOCK_MEDIUM_AND_ABOVE
        generation_config:
          temperature: 0.7
          top_p: 0.9
          max_output_tokens: 4096

  # Anthropic Claude Configuration
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    default_model: claude-3-5-sonnet-20241022
    models:
      claude-3-5-sonnet-20241022:
        max_tokens: 4096
        temperature: 0.7
        # Anthropic-specific parameter
        stop_sequences: ["\n\nHuman:"]
      claude-3-opus-20240229:
        max_tokens: 4096
        temperature: 0.8

  # Ollama Configuration (for local models)
  ollama:
    # Base URL for the Ollama server.
    base_url: http://localhost:11434
    # Default model to use. Assumes 'llama3.2' is available.
    default_model: llama3.2
    models:
      llama3.2:
        # Ollama-specific options
        options:
          num_ctx: 4096 # Context window size
          temperature: 0.8
      mistral:
        options:
          num_ctx: 8192
          temperature: 0.7

# -- MQTT Broker and Device Configuration --
mqtt:
  # Connection details for your MQTT broker.
  # Use environment variables for credentials.
  broker:
    host: localhost
    port: 1883
    # Set USER and PASSWORD via MQTT_USER and MQTT_PASSWORD env vars.
    user: ${MQTT_USER}
    password: ${MQTT_PASSWORD}

  # Home Assistant style auto-discovery.
  auto_discovery:
    enabled: true
    # The topic prefix for Home Assistant discovery messages.
    topic_prefix: homeassistant

  # Message filtering and performance settings
  filtering:
    enabled: true
    # Global rate limiting
    max_messages_per_second: 50
    per_device_messages_per_second: 5
    
    # Message deduplication cache
    message_cache_ttl: 300  # 5 minutes
    max_cache_entries: 1000
    
    # Discovery message filtering rules (processed in priority order)
    filter_rules:
      # Allow standard HA discovery for common device types
      - topic_pattern: "homeassistant/+/+/config"
        device_types: ["light", "switch", "sensor", "climate", "cover", "lock"]
        action: "allow"
        priority: 10
      
      # Block noisy device trackers by default
      - topic_pattern: "homeassistant/device_tracker/+/config"
        action: "deny"
        priority: 5
      
      # Block battery sensors from noisy devices
      - topic_pattern: "homeassistant/sensor/+_battery/config"
        action: "deny"
        priority: 3
    
    # Batch processing settings for performance
    discovery_batch_size: 10
    discovery_batch_timeout: 5.0  # seconds

  # Device registry persistence
  device_registry_file: data/mqtt_devices.json

  # List of explicitly configured devices.
  # This allows overriding auto-discovered devices or adding devices
  # that don't support auto-discovery.
  # IMPORTANT: Any 'devices' list in config.user.yaml will REPLACE this entire list.
  # This list is empty by default. See config.user.yaml for examples.
  devices: []
  # Example of a device structure:
  # devices:
  #   - name: "Living Room Lamp"
  #     # Unique ID for the device. If not provided, it's derived from the name.
  #     unique_id: "living_room_lamp_01"
  #     # Specific topics for this device
  #     state_topic: "home/livingroom/lamp/state"
  #     command_topic: "home/livingroom/lamp/set"
  #     # Override the global AI provider/model for this device
  #     ai_override:
  #       provider: openai
  #       model: gpt-3.5-turbo

# -- Performance and Metrics --
performance:
  # Enable or disable metrics collection.
  metrics_enabled: true
  # Metrics file storage location
  metrics_file: data/metrics.json
  # How often to flush metrics to disk (seconds)
  metrics_flush_interval: 60
  # How long to retain metrics data (days)
  metrics_retention_days: 30
  # Enable hourly rollup aggregation for efficiency
  metrics_rollup_enabled: true
  
  # Provider failover and circuit breaker settings
  failover_rules:
    # Try same provider's other models first before switching providers
    same_provider_first: true
    # Number of retries before switching providers
    max_retries_before_switch: 3
    # Number of failures before opening circuit breaker
    circuit_breaker_threshold: 5
    # Seconds to wait before attempting recovery
    recovery_timeout_seconds: 300
    
  # Performance scoring weights for provider selection
  performance_weight:
    latency: 0.3      # Response time importance
    error_rate: 0.4   # Reliability importance
    cost: 0.2         # Cost efficiency importance  
    success_rate: 0.1 # Overall success importance
  
  # Provider health monitoring settings
  health_monitoring:
    # How often to check provider health (seconds)
    check_interval_seconds: 60
    # Use lightweight ping instead of full inference
    lightweight_ping: true
  
  # Enable or disable caching of AI responses to reduce costs and latency.
  cache:
    enabled: true
    # Time-to-live for cache entries in seconds.
    ttl: 3600

# -- Service Configuration --
service:
  # API service configuration
  api:
    host: 0.0.0.0
    port: 8000
    # API key for accessing the service
    api_key: ${AICLEANER_API_KEY}
  # Home Assistant integration settings
  home_assistant:
    # Base URL for HA instance
    url: http://localhost:8123
    # Long-lived access token
    token: ${HA_TOKEN}