# AMD 8845HS + Radeon 780M Optimization Configuration
# CPU+iGPU Optimization Specialist Configuration

amd_optimization:
  # Hardware Configuration
  hardware:
    cpu_model: "AMD 8845HS"
    cpu_cores: 8
    cpu_threads: 16
    memory_total_gb: 64
    igpu_model: "Radeon 780M"
    igpu_compute_units: 12
    igpu_shaders: 768
    memory_bandwidth_gbps: 89.6
    
  # Provider Configuration
  llamacpp_amd:
    enabled: true
    priority: 1  # Highest priority for local processing
    
    # Model Configuration
    model_directory: "/data/models"
    default_model: "llava-7b-q4"
    
    # AMD 780M Specific Settings
    amd_780m:
      opencl_enabled: true
      vulkan_enabled: false
      gpu_layers: 20  # Starting point for 780M
      cpu_threads: 8  # Optimal for 8845HS
      context_size: 4096
      batch_size: 512
      memory_f16: true
      mlock: true
      numa: false
      
    # Performance Tuning
    performance:
      target_tokens_per_second: 10.0
      max_first_token_latency: 2.0
      min_success_rate: 0.95
      memory_safety_margin_gb: 4.0
      
    # Dynamic Optimization
    optimization:
      gpu_layer_step_size: 2
      min_gpu_layers: 0
      max_gpu_layers: 32
      performance_window_size: 10
      optimization_interval_seconds: 30
      auto_benchmark_on_startup: true
      
    # Model Profiles
    models:
      llava-7b-q4:
        file_name: "llava-v1.5-7b-q4_k_m.gguf"
        size_gb: 4.1
        target_tokens_per_second: 15.0
        min_memory_gb: 8.0
        optimal_gpu_layers: 16
        quantization: "Q4_K_M"
        capabilities: ["text_analysis", "image_analysis", "instruction_following"]
        
      llava-7b-q5:
        file_name: "llava-v1.5-7b-q5_k_m.gguf"
        size_gb: 5.1
        target_tokens_per_second: 12.0
        min_memory_gb: 10.0
        optimal_gpu_layers: 18
        quantization: "Q5_K_M"
        capabilities: ["text_analysis", "image_analysis", "instruction_following"]
        
      llava-13b-q4:
        file_name: "llava-v1.5-13b-q4_k_m.gguf"
        size_gb: 7.9
        target_tokens_per_second: 8.0
        min_memory_gb: 16.0
        optimal_gpu_layers: 24
        quantization: "Q4_K_M"
        capabilities: ["text_analysis", "image_analysis", "instruction_following", "advanced_reasoning"]
        
      llava-13b-q5:
        file_name: "llava-v1.5-13b-q5_k_m.gguf"
        size_gb: 9.8
        target_tokens_per_second: 6.0
        min_memory_gb: 20.0
        optimal_gpu_layers: 26
        quantization: "Q5_K_M"
        capabilities: ["text_analysis", "image_analysis", "instruction_following", "advanced_reasoning"]

# Provider Priority Configuration for AI Provider Manager
providers:
  llamacpp_amd:
    enabled: true
    priority: 1  # Highest priority - prefer local processing
    fallback_enabled: true
    weight: 1.0
    config:
      model_name: "llava-7b-q4"
      timeout_seconds: 60  # Longer timeout for model loading
      max_retries: 2
      health_check_interval: 300
      rate_limit_rpm: 200  # Generous for local models
      rate_limit_tpm: 100000
      daily_budget: 0.0  # No cost for local models
      
  ollama:
    enabled: true
    priority: 2  # Secondary local option
    fallback_enabled: true
    weight: 0.8
    config:
      base_url: "http://localhost:11434"
      model_name: "llava:13b"
      timeout_seconds: 30
      
  google:
    enabled: true
    priority: 3  # Cloud fallback for complex tasks
    fallback_enabled: true
    weight: 0.9
    config:
      model_name: "gemini-1.5-pro"
      
  openai:
    enabled: true
    priority: 4  # Cloud fallback
    fallback_enabled: true
    weight: 0.8
    config:
      model_name: "gpt-4-vision-preview"
      
  anthropic:
    enabled: true
    priority: 5  # Cloud fallback
    fallback_enabled: true
    weight: 0.7
    config:
      model_name: "claude-3-sonnet-20240229"

# Request Routing Configuration
routing:
  # Privacy-first routing
  privacy_mode:
    force_local: true
    allowed_providers: ["llamacpp_amd", "ollama"]
    
  # Vision request routing
  vision_requests:
    prefer_local: true
    local_providers: ["llamacpp_amd"]
    cloud_fallback: ["google", "openai"]
    
  # Code generation routing
  code_requests:
    prefer_providers: ["llamacpp_amd", "anthropic", "openai"]
    
  # Complex request routing
  complex_requests:
    min_prompt_length: 1000
    complexity_indicators:
      - "analyze"
      - "research" 
      - "comprehensive"
      - "detailed analysis"
      - "compare and contrast"
      - "in-depth"
      - "thorough examination"
      - "professional report"
      - "academic"
      - "scientific"
      - "technical specification"
      - "multi-step"
      - "complex problem"
      - "advanced"
      - "sophisticated"
    preferred_providers: ["google", "anthropic", "openai"]
    
# Performance Monitoring
monitoring:
  enabled: true
  metrics_collection_interval: 30  # seconds
  performance_history_size: 120  # 1 hour of data
  
  # Alert thresholds
  alerts:
    cpu_utilization_high: 90.0
    gpu_utilization_high: 95.0
    memory_usage_critical: 90.0
    tokens_per_second_low: 5.0
    first_token_latency_high: 5.0
    
  # Optimization triggers
  optimization:
    auto_optimize: true
    optimization_interval: 300  # 5 minutes
    performance_degradation_threshold: 0.8  # 80% of target performance
    
# Integration with AICleaner v3
aicleaner_integration:
  # When to use local vs cloud processing
  local_processing_preferences:
    privacy_sensitive_requests: true
    image_analysis_tasks: true
    simple_text_processing: true
    code_generation_tasks: true
    
  cloud_processing_preferences:
    complex_analysis_tasks: true
    research_intensive_tasks: true
    high_accuracy_requirements: true
    real_time_data_requirements: true
    
  # Fallback behavior
  fallback:
    max_local_failures: 3
    fallback_cooldown_seconds: 300
    auto_recovery: true
    
# Logging Configuration
logging:
  level: "INFO"
  amd_optimization_log: "/data/logs/amd_optimization.log"
  performance_log: "/data/logs/amd_performance.log"
  benchmark_log: "/data/logs/amd_benchmarks.log"